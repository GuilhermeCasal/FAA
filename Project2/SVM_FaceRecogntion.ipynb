{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-31 14:56:06.560917: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-31 14:56:06.561022: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-31 14:56:06.658990: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-31 14:56:06.825242: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-31 14:56:07.809187: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.309917</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.417355</td>\n",
       "      <td>0.442149</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>0.607438</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.690083</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665289</td>\n",
       "      <td>0.669421</td>\n",
       "      <td>0.652893</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.475207</td>\n",
       "      <td>0.132231</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.157025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.471074</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.557851</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>0.640496</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.710744</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.400826</td>\n",
       "      <td>0.491736</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>0.586777</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.698347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074380</td>\n",
       "      <td>0.132231</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.128099</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.140496</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.198347</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.243802</td>\n",
       "      <td>0.404959</td>\n",
       "      <td>0.483471</td>\n",
       "      <td>0.516529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652893</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.743802</td>\n",
       "      <td>0.764463</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.739669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.582645</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0.648760</td>\n",
       "      <td>0.690083</td>\n",
       "      <td>0.694215</td>\n",
       "      <td>0.714876</td>\n",
       "      <td>0.723140</td>\n",
       "      <td>0.731405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0.173554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.400826</td>\n",
       "      <td>0.495868</td>\n",
       "      <td>0.570248</td>\n",
       "      <td>0.632231</td>\n",
       "      <td>0.648760</td>\n",
       "      <td>0.640496</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.665289</td>\n",
       "      <td>0.698347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388430</td>\n",
       "      <td>0.396694</td>\n",
       "      <td>0.264463</td>\n",
       "      <td>0.099174</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.243802</td>\n",
       "      <td>0.247934</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.351240</td>\n",
       "      <td>0.301653</td>\n",
       "      <td>0.247934</td>\n",
       "      <td>0.247934</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.574380</td>\n",
       "      <td>0.628099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380165</td>\n",
       "      <td>0.334711</td>\n",
       "      <td>0.289256</td>\n",
       "      <td>0.285124</td>\n",
       "      <td>0.338843</td>\n",
       "      <td>0.404959</td>\n",
       "      <td>0.458678</td>\n",
       "      <td>0.487603</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.549587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.533058</td>\n",
       "      <td>0.607438</td>\n",
       "      <td>0.628099</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.632231</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.669421</td>\n",
       "      <td>0.673554</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.190083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.214876</td>\n",
       "      <td>0.219008</td>\n",
       "      <td>0.219008</td>\n",
       "      <td>0.223140</td>\n",
       "      <td>0.210744</td>\n",
       "      <td>0.202479</td>\n",
       "      <td>0.276859</td>\n",
       "      <td>0.400826</td>\n",
       "      <td>0.487603</td>\n",
       "      <td>0.549587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446281</td>\n",
       "      <td>0.392562</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.479339</td>\n",
       "      <td>0.524793</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.574380</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.603306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.516529</td>\n",
       "      <td>0.462810</td>\n",
       "      <td>0.280992</td>\n",
       "      <td>0.252066</td>\n",
       "      <td>0.247934</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.574380</td>\n",
       "      <td>0.615702</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.615702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276859</td>\n",
       "      <td>0.264463</td>\n",
       "      <td>0.293388</td>\n",
       "      <td>0.301653</td>\n",
       "      <td>0.293388</td>\n",
       "      <td>0.322314</td>\n",
       "      <td>0.322314</td>\n",
       "      <td>0.359504</td>\n",
       "      <td>0.355372</td>\n",
       "      <td>0.384298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 4096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0    0.309917  0.367769  0.417355  0.442149  0.528926  0.607438  0.657025   \n",
       "1    0.454545  0.471074  0.512397  0.557851  0.595041  0.640496  0.681818   \n",
       "2    0.318182  0.400826  0.491736  0.528926  0.586777  0.657025  0.681818   \n",
       "3    0.198347  0.194215  0.194215  0.194215  0.190083  0.190083  0.243802   \n",
       "4    0.500000  0.545455  0.582645  0.623967  0.648760  0.690083  0.694215   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "395  0.400826  0.495868  0.570248  0.632231  0.648760  0.640496  0.661157   \n",
       "396  0.367769  0.367769  0.351240  0.301653  0.247934  0.247934  0.367769   \n",
       "397  0.500000  0.533058  0.607438  0.628099  0.657025  0.632231  0.657025   \n",
       "398  0.214876  0.219008  0.219008  0.223140  0.210744  0.202479  0.276859   \n",
       "399  0.516529  0.462810  0.280992  0.252066  0.247934  0.367769  0.574380   \n",
       "\n",
       "         7         8         9     ...      4086      4087      4088  \\\n",
       "0    0.677686  0.690083  0.685950  ...  0.665289  0.669421  0.652893   \n",
       "1    0.702479  0.710744  0.702479  ...  0.136364  0.157025  0.136364   \n",
       "2    0.685950  0.702479  0.698347  ...  0.074380  0.132231  0.181818   \n",
       "3    0.404959  0.483471  0.516529  ...  0.652893  0.636364  0.657025   \n",
       "4    0.714876  0.723140  0.731405  ...  0.190083  0.161157  0.177686   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "395  0.636364  0.665289  0.698347  ...  0.388430  0.396694  0.264463   \n",
       "396  0.512397  0.574380  0.628099  ...  0.380165  0.334711  0.289256   \n",
       "397  0.669421  0.673554  0.702479  ...  0.194215  0.148760  0.152893   \n",
       "398  0.400826  0.487603  0.549587  ...  0.446281  0.392562  0.367769   \n",
       "399  0.615702  0.661157  0.615702  ...  0.276859  0.264463  0.293388   \n",
       "\n",
       "         4089      4090      4091      4092      4093      4094      4095  \n",
       "0    0.661157  0.475207  0.132231  0.148760  0.152893  0.161157  0.157025  \n",
       "1    0.148760  0.152893  0.152893  0.152893  0.152893  0.152893  0.152893  \n",
       "2    0.136364  0.128099  0.148760  0.144628  0.140496  0.148760  0.152893  \n",
       "3    0.685950  0.727273  0.743802  0.764463  0.752066  0.752066  0.739669  \n",
       "4    0.173554  0.177686  0.177686  0.177686  0.177686  0.173554  0.173554  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "395  0.099174  0.181818  0.243802  0.247934  0.161157  0.157025  0.136364  \n",
       "396  0.285124  0.338843  0.404959  0.458678  0.487603  0.512397  0.549587  \n",
       "397  0.161157  0.161157  0.173554  0.157025  0.177686  0.148760  0.190083  \n",
       "398  0.409091  0.479339  0.524793  0.545455  0.574380  0.590909  0.603306  \n",
       "399  0.301653  0.293388  0.322314  0.322314  0.359504  0.355372  0.384298  \n",
       "\n",
       "[400 rows x 4096 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces = np.load(\"olivetti_faces.npy\")\n",
    "faces_target = np.load(\"olivetti_faces_target.npy\")\n",
    "\n",
    "target_df = pd.DataFrame(faces_target)\n",
    "\n",
    "faces_reshape = faces.reshape((faces.shape[0],faces.shape[1]*faces.shape[2]))\n",
    "faces_df = pd.DataFrame(faces_reshape)\n",
    "faces_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.309917</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.417355</td>\n",
       "      <td>0.442149</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>0.607438</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.690083</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698347</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.632231</td>\n",
       "      <td>0.566116</td>\n",
       "      <td>0.524793</td>\n",
       "      <td>0.446281</td>\n",
       "      <td>0.371901</td>\n",
       "      <td>0.330579</td>\n",
       "      <td>0.305785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.342975</td>\n",
       "      <td>0.404959</td>\n",
       "      <td>0.438017</td>\n",
       "      <td>0.471074</td>\n",
       "      <td>0.553719</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0.669421</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.690083</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698347</td>\n",
       "      <td>0.673554</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.603306</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.442149</td>\n",
       "      <td>0.371901</td>\n",
       "      <td>0.338843</td>\n",
       "      <td>0.314050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.342975</td>\n",
       "      <td>0.417355</td>\n",
       "      <td>0.450413</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.574380</td>\n",
       "      <td>0.644628</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.690083</td>\n",
       "      <td>0.694215</td>\n",
       "      <td>0.694215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.644628</td>\n",
       "      <td>0.603306</td>\n",
       "      <td>0.570248</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>0.380165</td>\n",
       "      <td>0.338843</td>\n",
       "      <td>0.297521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.305785</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.471074</td>\n",
       "      <td>0.524793</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>0.652893</td>\n",
       "      <td>0.673554</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.690083</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.648760</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0.541322</td>\n",
       "      <td>0.471074</td>\n",
       "      <td>0.384298</td>\n",
       "      <td>0.347107</td>\n",
       "      <td>0.264463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.334711</td>\n",
       "      <td>0.421488</td>\n",
       "      <td>0.475207</td>\n",
       "      <td>0.582645</td>\n",
       "      <td>0.640496</td>\n",
       "      <td>0.665289</td>\n",
       "      <td>0.665289</td>\n",
       "      <td>0.673554</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.644628</td>\n",
       "      <td>0.644628</td>\n",
       "      <td>0.553719</td>\n",
       "      <td>0.487603</td>\n",
       "      <td>0.388430</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.334711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.206612</td>\n",
       "      <td>0.198347</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.206612</td>\n",
       "      <td>0.198347</td>\n",
       "      <td>0.198347</td>\n",
       "      <td>0.185950</td>\n",
       "      <td>0.516529</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.611570</td>\n",
       "      <td>0.611570</td>\n",
       "      <td>0.615702</td>\n",
       "      <td>0.640496</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.161157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.206612</td>\n",
       "      <td>0.202479</td>\n",
       "      <td>0.198347</td>\n",
       "      <td>0.214876</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.202479</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0.632231</td>\n",
       "      <td>0.628099</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.165289</td>\n",
       "      <td>0.169421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.214876</td>\n",
       "      <td>0.206612</td>\n",
       "      <td>0.223140</td>\n",
       "      <td>0.202479</td>\n",
       "      <td>0.214876</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.628099</td>\n",
       "      <td>0.648760</td>\n",
       "      <td>0.648760</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.495868</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.165289</td>\n",
       "      <td>0.173554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.202479</td>\n",
       "      <td>0.210744</td>\n",
       "      <td>0.210744</td>\n",
       "      <td>0.206612</td>\n",
       "      <td>0.202479</td>\n",
       "      <td>0.107438</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.566116</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.644628</td>\n",
       "      <td>0.665289</td>\n",
       "      <td>0.648760</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.487603</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.165289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.202479</td>\n",
       "      <td>0.206612</td>\n",
       "      <td>0.202479</td>\n",
       "      <td>0.214876</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.095041</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.710744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665289</td>\n",
       "      <td>0.669421</td>\n",
       "      <td>0.652893</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.475207</td>\n",
       "      <td>0.132231</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.157025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.309917  0.367769  0.417355  0.442149  0.528926  0.607438  0.657025   \n",
       "1   0.342975  0.404959  0.438017  0.471074  0.553719  0.623967  0.669421   \n",
       "2   0.342975  0.417355  0.450413  0.512397  0.574380  0.644628  0.677686   \n",
       "3   0.305785  0.409091  0.471074  0.524793  0.595041  0.652893  0.673554   \n",
       "4   0.334711  0.421488  0.475207  0.582645  0.640496  0.665289  0.665289   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "59  0.206612  0.198347  0.194215  0.206612  0.198347  0.198347  0.185950   \n",
       "60  0.206612  0.202479  0.198347  0.214876  0.190083  0.202479  0.173554   \n",
       "61  0.214876  0.206612  0.223140  0.202479  0.214876  0.157025  0.181818   \n",
       "62  0.202479  0.210744  0.210744  0.206612  0.202479  0.107438  0.190083   \n",
       "63  0.202479  0.206612  0.202479  0.214876  0.144628  0.095041  0.227273   \n",
       "\n",
       "          7         8         9   ...        54        55        56        57  \\\n",
       "0   0.677686  0.690083  0.685950  ...  0.698347  0.677686  0.657025  0.632231   \n",
       "1   0.685950  0.690083  0.685950  ...  0.698347  0.673554  0.657025  0.636364   \n",
       "2   0.690083  0.694215  0.694215  ...  0.685950  0.677686  0.657025  0.644628   \n",
       "3   0.677686  0.685950  0.685950  ...  0.690083  0.677686  0.657025  0.648760   \n",
       "4   0.673554  0.681818  0.677686  ...  0.702479  0.685950  0.661157  0.644628   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "59  0.516529  0.677686  0.657025  ...  0.611570  0.611570  0.615702  0.640496   \n",
       "60  0.528926  0.681818  0.677686  ...  0.623967  0.632231  0.628099  0.657025   \n",
       "61  0.545455  0.685950  0.702479  ...  0.628099  0.648760  0.648760  0.661157   \n",
       "62  0.566116  0.685950  0.702479  ...  0.644628  0.665289  0.648760  0.661157   \n",
       "63  0.590909  0.685950  0.710744  ...  0.665289  0.669421  0.652893  0.661157   \n",
       "\n",
       "          58        59        60        61        62        63  \n",
       "0   0.566116  0.524793  0.446281  0.371901  0.330579  0.305785  \n",
       "1   0.603306  0.512397  0.442149  0.371901  0.338843  0.314050  \n",
       "2   0.603306  0.570248  0.433884  0.380165  0.338843  0.297521  \n",
       "3   0.623967  0.541322  0.471074  0.384298  0.347107  0.264463  \n",
       "4   0.644628  0.553719  0.487603  0.388430  0.367769  0.334711  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "59  0.512397  0.177686  0.144628  0.157025  0.152893  0.161157  \n",
       "60  0.500000  0.161157  0.157025  0.161157  0.165289  0.169421  \n",
       "61  0.495868  0.148760  0.157025  0.152893  0.165289  0.173554  \n",
       "62  0.487603  0.136364  0.152893  0.148760  0.161157  0.165289  \n",
       "63  0.475207  0.132231  0.148760  0.152893  0.161157  0.157025  \n",
       "\n",
       "[64 rows x 64 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.array(faces_df.iloc[0,:]).reshape((64,64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (320, 4096)\n",
      "y_train shape:(320,)\n"
     ]
    }
   ],
   "source": [
    "X, X_test, y, y_test=train_test_split(faces_df.values, faces_target, test_size=0.2, stratify=faces_target,shuffle=True, random_state=42)\n",
    "print(\"X_train shape:\",X.shape)\n",
    "print(\"y_train shape:{}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model (X,y, lda_object):\n",
    "    num_folds = 4\n",
    "\n",
    "    # Create a stratified k-fold cross-validation object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=num_folds,random_state=42,shuffle=True)\n",
    "    lda = lda_object\n",
    "    accuracy1 = 0\n",
    "    accuracy2 = 0\n",
    "    initial_theta = None\n",
    "    # Perform stratified k-fold cross-validation\n",
    "    for train_index, test_index in stratified_kfold.split(X, y):\n",
    "        X_train, X_val = X[train_index], X[test_index]\n",
    "        y_train, y_val = y[train_index], y[test_index]\n",
    "\n",
    "        lda = lda_object\n",
    "\n",
    "        #if initial_theta is not None:\n",
    "            #lda.coef_ = initial_theta\n",
    "        # Fit the model on the training data\n",
    "        history = lda.fit(X_train, y_train)\n",
    "\n",
    "        initial_theta = lda.coef_\n",
    "\n",
    "        y_pred = lda.predict(X_val)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy1 += accuracy_score(y_val, y_pred)\n",
    "        \n",
    "        y_pred = lda.predict(X_train)\n",
    "        accuracy2 += accuracy_score(y_train, y_pred)\n",
    "    print(f\"Accuracy val: {accuracy1/4:.2f}\")\n",
    "    print(f\"Accuracy train: {accuracy2/4:.2f}\")\n",
    "    return lda\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: 0.96\n",
      "Accuracy train: 1.00\n",
      "Accuracy test: 1.00\n"
     ]
    }
   ],
   "source": [
    "lda_object = LinearDiscriminantAnalysis()\n",
    "lda = lda_model(X,y,lda_object)\n",
    "\n",
    "\n",
    "lda.fit(X, y)\n",
    "y_pred = lda.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy test: {accuracy:.2f}\")\n",
    "lda = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Best Parameters:  {'n_components': 1}\n",
      "Best Accuracy:  0.96875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#lda = LinearDiscriminantAnalysis(solver='svd', shrinkage=None, n_components=None, store_covariance=False)\n",
    "param_grid = {\n",
    "    'n_components': [1, 2, 3, 4, 5, 10, None], }\n",
    "grid_search = GridSearchCV(lda, param_grid, cv=4,scoring=\"accuracy\",verbose=1,n_jobs=-1)  # You can adjust the number of folds (cv) as needed\n",
    "\n",
    "# Perform the grid search on your data\n",
    "grid_search.fit(X, y)\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Accuracy: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lda_object \u001b[38;5;241m=\u001b[39m LinearDiscriminantAnalysis(shrinkage\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, solver \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlsqr\u001b[39m\u001b[38;5;124m'\u001b[39m,n_components \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m lda \u001b[38;5;241m=\u001b[39m lda_model(X,y,lda_object)\n\u001b[1;32m      5\u001b[0m lda\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[1;32m      6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m lda\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m, in \u001b[0;36mlda_model\u001b[0;34m(X, y, lda_object)\u001b[0m\n\u001b[1;32m     15\u001b[0m lda \u001b[38;5;241m=\u001b[39m lda_object\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#if initial_theta is not None:\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#lda.coef_ = initial_theta\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Fit the model on the training data\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m history \u001b[38;5;241m=\u001b[39m lda\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     22\u001b[0m initial_theta \u001b[38;5;241m=\u001b[39m lda\u001b[38;5;241m.\u001b[39mcoef_\n\u001b[1;32m     24\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m lda\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[0;32m~/anaconda3/envs/firstEnv/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/firstEnv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:630\u001b[0m, in \u001b[0;36mLinearDiscriminantAnalysis.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solve_svd(X, y)\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlsqr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solve_lstsq(\n\u001b[1;32m    631\u001b[0m         X,\n\u001b[1;32m    632\u001b[0m         y,\n\u001b[1;32m    633\u001b[0m         shrinkage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshrinkage,\n\u001b[1;32m    634\u001b[0m         covariance_estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_estimator,\n\u001b[1;32m    635\u001b[0m     )\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meigen\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solve_eigen(\n\u001b[1;32m    638\u001b[0m         X,\n\u001b[1;32m    639\u001b[0m         y,\n\u001b[1;32m    640\u001b[0m         shrinkage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshrinkage,\n\u001b[1;32m    641\u001b[0m         covariance_estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_estimator,\n\u001b[1;32m    642\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/firstEnv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:408\u001b[0m, in \u001b[0;36mLinearDiscriminantAnalysis._solve_lstsq\u001b[0;34m(self, X, y, shrinkage, covariance_estimator)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans_ \u001b[38;5;241m=\u001b[39m _class_means(X, y)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_ \u001b[38;5;241m=\u001b[39m _class_cov(\n\u001b[1;32m    406\u001b[0m     X, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriors_, shrinkage, covariance_estimator\n\u001b[1;32m    407\u001b[0m )\n\u001b[0;32m--> 408\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_ \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mlstsq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans_\u001b[38;5;241m.\u001b[39mT)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT)) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriors_\n\u001b[1;32m    411\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/firstEnv/lib/python3.11/site-packages/scipy/linalg/_basic.py:1282\u001b[0m, in \u001b[0;36mlstsq\u001b[0;34m(a, b, cond, overwrite_a, overwrite_b, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m real_data:\n\u001b[1;32m   1281\u001b[0m     lwork, iwork \u001b[38;5;241m=\u001b[39m _compute_lwork(lapack_lwork, m, n, nrhs, cond)\n\u001b[0;32m-> 1282\u001b[0m     x, s, rank, info \u001b[38;5;241m=\u001b[39m lapack_func(a1, b1, lwork,\n\u001b[1;32m   1283\u001b[0m                                    iwork, cond, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# complex data\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m     lwork, rwork, iwork \u001b[38;5;241m=\u001b[39m _compute_lwork(lapack_lwork, m, n,\n\u001b[1;32m   1286\u001b[0m                                          nrhs, cond)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lda_object = LinearDiscriminantAnalysis(shrinkage= 'auto', solver = 'lsqr',n_components =1)\n",
    "lda = lda_model(X,y,lda_object)\n",
    "\n",
    "\n",
    "lda.fit(X, y)\n",
    "y_pred = lda.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy test: {accuracy:.2f}\")\n",
    "lda = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(faces_df.values, faces_target, test_size=0.2, stratify=faces_target,shuffle=True, random_state=42)\n",
    "X_lda_train = lda_object.fit_transform(X_train, y_train)\n",
    "X_lda_test = lda_object.transform(X_test)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have X_train, X_test, y_train, and y_test\n",
    "\n",
    "# Select only 10 people for visualization\n",
    "selected_people = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "selected_indices_train = [i for i, label in enumerate(y_train) if label in selected_people]\n",
    "\n",
    "# Filter data for the selected people\n",
    "X_selected_train = X_train[selected_indices_train]\n",
    "y_selected_train = y_train[selected_indices_train]\n",
    "\n",
    "# Fit LDA model for dimensionality reduction\n",
    "lda_object = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda_train = lda_object.fit_transform(X_selected_train, y_selected_train)\n",
    "plt.figure(figsize=(10,10))\n",
    "# Plot the 2D scatter plot for the selected people in the training data\n",
    "for class_label in set(y_selected_train):\n",
    "    plt.scatter(X_lda_train[y_selected_train == class_label, 0],\n",
    "                X_lda_train[y_selected_train == class_label, 1],\n",
    "                label=f'Person {class_label}')\n",
    "\n",
    "plt.title('2D Scatter Plot of Selected Training Data with LDA Dimensionality Reduction')\n",
    "plt.xlabel('LDA Component 1')\n",
    "plt.ylabel('LDA Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(1, figsize=(12, 8))\n",
    "sns.heatmap(cm, fmt='g')  # annot=True to display the counts, fmt='g' to format as integers\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True Value')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LDA transformation to the training and testing data\n",
    "lda_object = LinearDiscriminantAnalysis(n_components = 9)\n",
    "lda = lda_model(X_pca_train,y_pca_train,lda_object)\n",
    "\n",
    "X_train_lda = lda.transform(X_pca_train)\n",
    "X_test_lda = lda.transform(X_pca_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
